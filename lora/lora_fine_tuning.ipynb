{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed9c236",
   "metadata": {},
   "source": [
    "# **LoRAfrica: Scaling LLM Fine Tuning for African History**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9684865",
   "metadata": {},
   "source": [
    "## **Fine-tuning with LoRA Adaption**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2f3ab-de87-49da-ae5c-61a8241aec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import pipeline\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from bert_score import score as bert_score\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db352c1-3b47-4eca-ac1b-593de51a71ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 2114\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the African History QA Dataset\n",
    "full_dataset = load_dataset(\"DannyAI/African-History-QA-Dataset\")\n",
    "full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ea294",
   "metadata": {},
   "source": [
    "### **Account Sign in & setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca86caa-a167-4e4f-a406-b5f10d0c9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face Hub\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b1752-dae5-4da7-875a-67490b02911a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in to Weights & Biases\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71e8e0-6616-458d-a166-bf67903ab2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20260123_162556-ivh3ij4s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dannyai-danny-the-analyst/phi4_african_history/runs/ivh3ij4s' target=\"_blank\">phi4_african_history_lora</a></strong> to <a href='https://wandb.ai/dannyai-danny-the-analyst/phi4_african_history' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dannyai-danny-the-analyst/phi4_african_history' target=\"_blank\">https://wandb.ai/dannyai-danny-the-analyst/phi4_african_history</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dannyai-danny-the-analyst/phi4_african_history/runs/ivh3ij4s' target=\"_blank\">https://wandb.ai/dannyai-danny-the-analyst/phi4_african_history/runs/ivh3ij4s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dannyai-danny-the-analyst/phi4_african_history/runs/ivh3ij4s?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3720170c10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a new W&B run\n",
    "wandb.init(project=\"phi4_african_history\", name=\"phi4_african_history_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326ea2b",
   "metadata": {},
   "source": [
    "### **Model Setup & Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49efeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, validation and test data\n",
    "train_data = full_dataset[\"train\"]\n",
    "val_data = full_dataset[\"validation\"]\n",
    "test_data = full_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79373a95-78ba-4828-9f03-8d7504dc9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model_id and output_dir\n",
    "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
    "output_dir = \"./phi4_african_history_lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffedbd6-5f4f-4ed9-9468-ea1991bb3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(model_id)\n",
    "tokeniser.pad_token = tokeniser.eos_token\n",
    "\n",
    "# load model\n",
    "model  = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = \"auto\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    trust_remote_code = False\n",
    ")\n",
    "\n",
    "# lora settings\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout = 0.05, # dataset is small, hence a low dropout value\n",
    "    bias = \"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Lora model\n",
    "model = get_peft_model(model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321cf05a-ad0e-4876-a7fd-e7a381d9df8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 3,837,594,624 || trainable%: 0.0410\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2e8db",
   "metadata": {},
   "source": [
    "### **Masked tokenisation and Collator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ed268-7806-4934-8188-9e702d4f08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisation(example)->dict:\n",
    "    \"\"\"\n",
    "    Tokenizes the input example by applying the chat template.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":\"You are a helpful AI assistant specialised in African history which gives concise answers to questions asked\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":example[\"question\"]\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\":example[\"answer\"]\n",
    "        }\n",
    "    ]\n",
    "    full_text = tokeniser.apply_chat_template(messages,tokenize=False)\n",
    "    return tokeniser(full_text, truncation=True, \n",
    "                     max_length=2048, \n",
    "                     add_special_tokens=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b98c4-2625-4c6f-baf9-539d50fb098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "train_dataset = train_data.map(tokenisation)\n",
    "val_dataset = val_data.map(tokenisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3525207c-ab73-4041-b8d6-48a46a803f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2114\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a11cbe5-60de-4451-a033-000cda45d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator to mask inputs before assistant response\n",
    "class AssistantMaskingCollator():\n",
    "    def __init__(self,tokeniser):\n",
    "        self.tokeniser = tokeniser\n",
    "        self.assistant_header = tokeniser.encode(\"<|assistant|>\\n\", add_special_tokens=False)\n",
    "\n",
    "    def __call__(self,features):\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"]) for f in features]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids,batch_first=True,padding_value=self.tokeniser.pad_token_id)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            # Find where assistant response starts\n",
    "            found = False\n",
    "            for j in range(len(input_ids[i]) - len(self.assistant_header) + 1):\n",
    "                if input_ids[i][j : j + len(self.assistant_header)].tolist() == self.assistant_header:\n",
    "                    # Mask everything before and including the header\n",
    "                    labels[i, : j + len(self.assistant_header)] = -100\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            # If for some reason header isn't found, mask everything to be safe\n",
    "            if not found:\n",
    "                labels[i, :] = -100\n",
    "            \n",
    "            # Mask actual padding tokens\n",
    "            labels[i][input_ids[i] == self.tokeniser.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": input_ids.ne(self.tokeniser.pad_token_id).long(),\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a027d919-cf1f-408a-b8d9-f41c941b5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = AssistantMaskingCollator(tokeniser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76f317",
   "metadata": {},
   "source": [
    "### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48601ae-3e1e-442c-b0df-6a5d126ee612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "# STF can also be used here\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    bf16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ebca2-b33c-425d-9552-c2792200b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer with training arguments, datasets, model, and data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3626cb7-796b-42cc-9e69-1acae6335c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2650' max='2650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2650/2650 22:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.643900</td>\n",
       "      <td>1.650120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.548300</td>\n",
       "      <td>1.577856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.581000</td>\n",
       "      <td>1.551598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.578900</td>\n",
       "      <td>1.538108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.498800</td>\n",
       "      <td>1.528269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.401300</td>\n",
       "      <td>1.518312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>1.513678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.436400</td>\n",
       "      <td>1.506603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.545600</td>\n",
       "      <td>1.504393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.439800</td>\n",
       "      <td>1.502365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.452100</td>\n",
       "      <td>1.500665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.466000</td>\n",
       "      <td>1.494793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.408300</td>\n",
       "      <td>1.493954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.508900</td>\n",
       "      <td>1.493219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.487500</td>\n",
       "      <td>1.493616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.383300</td>\n",
       "      <td>1.489923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.534100</td>\n",
       "      <td>1.489187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.468800</td>\n",
       "      <td>1.489143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.405100</td>\n",
       "      <td>1.488410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.509100</td>\n",
       "      <td>1.487043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.435800</td>\n",
       "      <td>1.488957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.434400</td>\n",
       "      <td>1.487890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.416800</td>\n",
       "      <td>1.488166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.416600</td>\n",
       "      <td>1.487361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.439200</td>\n",
       "      <td>1.487180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>1.486632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2650, training_loss=1.4904400303678693, metrics={'train_runtime': 1365.2341, 'train_samples_per_second': 15.485, 'train_steps_per_second': 1.941, 'total_flos': 3.851040433792205e+16, 'train_loss': 1.4904400303678693, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5d4f0-482d-4b61-9c40-5431baf49a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bb254-37af-43df-9e3e-f09cb176ec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./phi4_african_history_lora\n"
     ]
    }
   ],
   "source": [
    "# Print a message indicating the model has been saved\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d70c8",
   "metadata": {},
   "source": [
    "### **Pushing to Huggingface Hub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01558be-eda0-46eb-a517-06f635bf1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN=\"token_here\"  # replace with your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cfb31d-40ea-43a1-b310-675a99f8be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    model,output_dir\n",
    ")\n",
    "\n",
    "# Push to Huggingface Hub\n",
    "username = \"DannyAI\" # replace with your Hugging Face username\n",
    "hf_project_name= \"phi4_african_history_lora\" # replace with your desired repo name\n",
    "repo_id = f\"{username}/{hf_project_name}\"\n",
    "lora_model.push_to_hub(repo_id,exist_ok=True,token=HF_TOKEN)\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_id)\n",
    "tokeniser.push_to_hub(repo_id,exist_ok=True,token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba1209",
   "metadata": {},
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bedc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory for inference\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44a053-3e39-4280-8148-77c16283c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7235c9-f52a-42c0-acba-110c66847495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Predictions on Test Set ---\n",
      "\n",
      "Sample Q: How did European traders impact the textile industry in the Kingdom of Kongo?\n",
      "Sample A (Lora Model): European traders introduced new textile materials and techniques, which were adopted and adapted by local weavers.\n",
      "Sample A (Ref): European traders, particularly the Portuguese, played a significant role in the textile industry in the Kingdom of Kongo, with the Portuguese trading raffia cloth and other textiles with the kingdom and re-exporting them to other regions.\n",
      "\n",
      "\n",
      "Sample Q: What is the significance of African feminist scholarly activism in contemporary resistance movements?\n",
      "Sample A (Lora Model): African feminist scholarly activism is significant in contemporary resistance movements as it provides a critical framework for understanding and addressing the specific challenges faced by African women in the context of global capitalism, neocolonialism, and patriarchal structures.\n",
      "Sample A (Ref): Contemporary African feminist scholarly activism has become a crucial component of resistance movements, developing unique theoretical frameworks like Stiwanism (coined by Omolara Ogundipe-Leslie) that address the intersection of gender, colonialism, and African cultural contexts. Modern African feminist scholars are actively challenging Western feminist paradigms while addressing issues specific to African women's experiences. Their work combines academic research with grassroots activism, focusing on decolonial approaches and indigenous knowledge systems in addressing gender inequality.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating BERTScore ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6e663acad64f649df40a0848dd9e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a4df3708404d2bad834032b05deced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.54 seconds, 183.68 sentences/sec\n",
      "\n",
      "Final Evaluation Results:\n",
      "Average BERTScore F1: 0.9075\n"
     ]
    }
   ],
   "source": [
    "# The pipeline handles chat templates and decoding automatically\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lora_model,\n",
    "    tokenizer=tokeniser,\n",
    ")\n",
    "\n",
    "def generate_answer(question)->str:\n",
    "    \"\"\"Generates an answer for the given question using the fine-tuned LoRA model.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialised in African history which gives concise answers to questions asked.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # pipeline() returns a list of dicts; return_full_text=False gives only the assistant's reply\n",
    "    output = generator(\n",
    "        messages, \n",
    "        max_new_tokens=2048, \n",
    "        temperature=0.1, \n",
    "        do_sample=False,\n",
    "        return_full_text=False\n",
    "    )\n",
    "    return output[0]['generated_text'].strip()\n",
    "\n",
    "# Generate predictions on the test set\n",
    "print(\"--- Generating Predictions on Test Set ---\")\n",
    "test_predictions = []\n",
    "# Assuming test_data is a list of dicts with \"question\" and \"answer\" keys\n",
    "test_references = [item[\"answer\"] for item in test_data]\n",
    "\n",
    "for i, item in enumerate(test_data):\n",
    "    pred = generate_answer(item[\"question\"])\n",
    "    test_predictions.append(pred)\n",
    "    \n",
    "    if i < 2: # Sample output for verification\n",
    "        print(f\"\\nSample Q: {item['question']}\")\n",
    "        print(f\"Sample A (Lora Model): {pred}\")\n",
    "        print(f\"Sample A (Ref): {item['answer']}\\n\")\n",
    "\n",
    "# Metrics Calculation using BERTScore\n",
    "print(\"--- Calculating BERTScore ---\")\n",
    "# P = Precision, R = Recall, F1 = F1 Score\n",
    "P, R, F1 = bert_score(test_predictions, test_references, lang=\"en\", verbose=True)\n",
    "\n",
    "avg_f1 = F1.mean().item()\n",
    "print(f\"\\nFinal Evaluation Results:\")\n",
    "print(f\"Average BERTScore F1: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c118c1af-e70b-480f-9e3f-97a87ae3d6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What role do international investments play in African e-commerce development?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['question'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fd6403d-4f5d-4310-86e1-d7f5e7e92c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"International investments, such as Amazon's entry into South Africa and various venture capital investments in platforms like Jumia, are crucial for the development of African e-commerce, bringing technology, expertise, and capital to the market.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['answer'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ddf747f-c2a6-4c53-9a44-675450529177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'International investments are crucial for African e-commerce development, providing necessary capital, technology, and expertise to scale operations and expand market reach.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64457bc4-5063-40f9-a6a2-e9bad6b3404c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Final_Test_BERTScore</td><td>▁</td></tr><tr><td>eval/loss</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▃▅▅▄▂▁▆▆▃▃▄▃▃▆▇█▃▄▃▄▄▃▃▅▅</td></tr><tr><td>eval/samples_per_second</td><td>█▆▄▄▅▇█▃▃▆▆▅▆▆▃▂▁▆▄▆▅▅▆▆▄▄</td></tr><tr><td>eval/steps_per_second</td><td>█▆▄▄▄▇█▃▃▆▆▅▆▆▃▂▁▆▄▆▅▅▆▆▄▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▂▃▄▃▄▄▄▄▆▅▇▇▆▅▆▅▆▆▅▅▆█▆▇▆█▇▆▆▆▆▆▇▆▆█▆█</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▅▄▄▄▄▃▄▄▅▄▃▂▃▃▃▃▃▄▃▂▂▂▂▄▃▂▂▃▂▂▁▂▃▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Final_Test_BERTScore</td><td>0.90687</td></tr><tr><td>eval/loss</td><td>1.48851</td></tr><tr><td>eval/runtime</td><td>2.9442</td></tr><tr><td>eval/samples_per_second</td><td>67.931</td></tr><tr><td>eval/steps_per_second</td><td>8.491</td></tr><tr><td>total_flos</td><td>3.851040433792205e+16</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>2650</td></tr><tr><td>train/grad_norm</td><td>8.32305</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.4934</td></tr><tr><td>train_loss</td><td>1.49071</td></tr><tr><td>train_runtime</td><td>1294.8986</td></tr><tr><td>train_samples_per_second</td><td>16.326</td></tr><tr><td>train_steps_per_second</td><td>2.046</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">phi3_lora_masked_finetune</strong> at: <a href='https://wandb.ai/dannyai-danny-the-analyst/phi3_african_history/runs/lvl01k50' target=\"_blank\">https://wandb.ai/dannyai-danny-the-analyst/phi3_african_history/runs/lvl01k50</a><br> View project at: <a href='https://wandb.ai/dannyai-danny-the-analyst/phi3_african_history' target=\"_blank\">https://wandb.ai/dannyai-danny-the-analyst/phi3_african_history</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260123_103007-lvl01k50/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finalize W&B\n",
    "wandb.log({\"Final_Test_BERTScore\": avg_f1})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
